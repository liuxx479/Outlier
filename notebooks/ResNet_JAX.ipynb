{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-neural-network.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMxhfoRyPL7jt76YsGVCcTp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuxx479/Outlier/blob/master/notebooks/ResNet_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiUibhYRCet3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "01aa1018-66b3-41fa-c0df-b5a4513c3cde"
      },
      "source": [
        "from __future__ import print_function, division, absolute_import\n",
        "\n",
        "import numpy.random as npr\n",
        "import os\n",
        "\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "import jax.numpy as np\n",
        "from jax.config import config\n",
        "from jax import jit, grad, random\n",
        "from jax.experimental import optimizers\n",
        "from jax.experimental import stax\n",
        "import tensorflow as tf\n",
        "from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,\n",
        "                                   FanOut, Flatten, GeneralConv, Identity,\n",
        "                                   MaxPool, Relu, LogSoftmax)\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "%pylab inline\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "!pip install --quiet git+https://github.com/liuxx479/Outlier.git\n",
        "from outlier.datasets import gaussian_convergence\n",
        "\n",
        "\n",
        "########### JL note: mostly following https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb\n",
        "########### https://github.com/google/jax/blob/master/examples/resnet50.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['np', 'random']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for Outlier (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYH51ZKJU7d3",
        "colab_type": "code",
        "outputId": "e3f10494-fac4-4004-ef7c-4ec16d2a38e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "######### mount google drive file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = '/content/drive/My Drive'\n",
        "data_path = os.path.join(root,'tensorflow_datasets/')\n",
        "model_path= os.path.join(root,'models/')\n",
        "\n",
        "if not os.path.isdir(model_path):\n",
        "    os.makedirs(model_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZHqV3oFXM0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## load data from gdrive, if need to downlaod again, set download=1\n",
        "VERSION = tfds.core.Version('0.1.0')\n",
        "dataset, info = tfds.load(name=\"gaussian_convergence\", download=0, split=\"train\", data_dir=data_path, with_info=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zginT0txXOjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8cfc130-f152-44a8-c100-a5ca3f69642e"
      },
      "source": [
        "print ('data size:',info.splits['train'].num_examples)\n",
        "data_size=info.splits['train'].num_examples"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data size: 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xwVxsWfVP2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for example in dataset.take(1):  #take 1 for test\n",
        "#   kappa_map, params = example[\"map\"], example[\"params\"]\n",
        "#   imshow(kappa_map)\n",
        "#   print(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPron-dAF0Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "S8_gen= lambda params: np.sqrt(params.T[0]/0.3)*params.T[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nx6qUOoxkgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ JL: resnet architecture, mostly unchanged, other than removed bunch of idensity matrices\n",
        "\n",
        "def ConvBlock(kernel_size, filters, strides=(2, 2)):\n",
        "  ks = kernel_size\n",
        "  filters1, filters2, filters3 = filters\n",
        "  Main = stax.serial(\n",
        "      Conv(filters1, (1, 1), strides), BatchNorm(), Relu,\n",
        "      Conv(filters2, (ks, ks), padding='SAME'), BatchNorm(), Relu,\n",
        "      Conv(filters3, (1, 1)), BatchNorm())\n",
        "  Shortcut = stax.serial(Conv(filters3, (1, 1), strides), BatchNorm())\n",
        "  return stax.serial(FanOut(2), stax.parallel(Main, Shortcut), FanInSum, Relu)\n",
        "\n",
        "def IdentityBlock(kernel_size, filters):\n",
        "  ks = kernel_size\n",
        "  filters1, filters2 = filters\n",
        "  def make_main(input_shape):\n",
        "    # the number of output channels depends on the number of input channels\n",
        "    return stax.serial(\n",
        "        Conv(filters1, (1, 1)), BatchNorm(), Relu,\n",
        "        Conv(filters2, (ks, ks), padding='SAME'), BatchNorm(), Relu,\n",
        "        Conv(input_shape[3], (1, 1)), BatchNorm())\n",
        "  Main = stax.shape_dependent(make_main)\n",
        "  return stax.serial(FanOut(2), stax.parallel(Main, Identity), FanInSum, Relu)\n",
        "\n",
        "# ResNet architectures compose layers and ResNet blocks\n",
        "def ResNet50(num_classes):\n",
        "  ########## JL to do: maybe remove some IdentityBlocks..\n",
        "  return stax.serial(\n",
        "      GeneralConv(('HWCN', 'OIHW', 'NHWC'), 64, (7, 7), (2, 2), 'SAME'),\n",
        "      BatchNorm(), \n",
        "      Relu, \n",
        "      MaxPool((3, 3), strides=(2, 2)),\n",
        "      ConvBlock(3, [64, 64, 256], strides=(1, 1)),\n",
        "      IdentityBlock(3, [64, 64]),\n",
        "      IdentityBlock(3, [64, 64]),\n",
        "      # ConvBlock(3, [128, 128, 512]),\n",
        "      # IdentityBlock(3, [128, 128]),\n",
        "      # IdentityBlock(3, [128, 128]),\n",
        "      # IdentityBlock(3, [128, 128]),\n",
        "      ConvBlock(3, [256, 256, 1024]),\n",
        "      IdentityBlock(3, [256, 256]),\n",
        "      # IdentityBlock(3, [256, 256]),\n",
        "      # IdentityBlock(3, [256, 256]),\n",
        "      # IdentityBlock(3, [256, 256]),\n",
        "      # IdentityBlock(3, [256, 256]),\n",
        "      ConvBlock(3, [512, 512, 2048]),\n",
        "      # IdentityBlock(3, [512, 512]),\n",
        "      IdentityBlock(3, [512, 512]),\n",
        "      AvgPool((7, 7)), \n",
        "      Flatten, \n",
        "      Dense(num_classes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGQO8HzF1Dt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import random\n",
        "rng_key = random.PRNGKey(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w359K7IaLuU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(params, batch):\n",
        "  kappa_map, params_true = batch\n",
        "  params_pred = predict_fun(params, kappa_map)\n",
        "  #out1 = np.sum((S8_gen(params_pred) - S8_gen(params_true))**2) ## controls S8\n",
        "  out2 = np.sum((params_pred - params_true)**2) ## controls individual params\n",
        "  out2 = np.mean((params_pred - params_true)**2)\n",
        "  return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SsrmV6paRoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########### hack from: https://github.com/google/jax/issues/2116\n",
        "from jax.tree_util import pytree\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "\n",
        "suffix = '.pickle'\n",
        "\n",
        "def savemodel(data: pytree, path: Union[str, Path], overwrite: bool = False):\n",
        "    path = Path(path)\n",
        "    if path.suffix != suffix:\n",
        "        path = path.with_suffix(suffix)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if path.exists():\n",
        "        if overwrite:\n",
        "            path.unlink()\n",
        "        else:\n",
        "            raise RuntimeError(f'File {path} already exists.')\n",
        "    with open(path, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "\n",
        "def loadmodel(path: Union[str, Path]) -> pytree:\n",
        "    path = Path(path)\n",
        "    if not path.is_file():\n",
        "        raise ValueError(f'Not a file: {path}')\n",
        "    if path.suffix != suffix:\n",
        "        raise ValueError(f'Not a {suffix} file: {path}')\n",
        "    with open(path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K06x5xl_3Mw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####### prepare training sets\n",
        "\n",
        "batch_size = 64 ### number of maps per batch\n",
        "valid_size = 128 #int(0.1*data_size)\n",
        "test_size = int(0.1*data_size)\n",
        "train_size = data_size - valid_size -test_size #int(0.8*data_size)\n",
        "\n",
        "dataset.shuffle(1000)\n",
        "valid_batch = dataset.take(valid_size).batch(valid_size)\n",
        "test_batch = dataset.skip(valid_size).take(test_size).batch(test_size)\n",
        "train_batch = dataset.skip(valid_size+test_size).repeat().batch(batch_size)\n",
        "\n",
        "def preprocess(dataset, augment=1):\n",
        "  images=dataset['map']\n",
        "  images=tf.transpose(images, (1,2,0)) ##### move the batch size to last axis\n",
        "  # print ('preproess shape', images.numpy().shape)\n",
        "  if augment:\n",
        "    images=tf.image.random_flip_left_right(images)\n",
        "    images=tf.image.random_flip_up_down(images)\n",
        "    images=tf.image.rot90(images, k=randint(4)) #### rotate an image in a random angle\n",
        "  images=tf.expand_dims(images,-2) ## expand the dimension to include channel\n",
        "  # images=tf.transpose(images, (1,2,3,0))\n",
        "  # print ('post processing shape',images.numpy().shape)\n",
        "  return images.numpy(), dataset['params'].numpy()\n",
        "\n",
        "for itest in valid_batch:\n",
        "  valid_batch = preprocess(itest, augment=0)\n",
        "  valid_maps, params_valid = valid_batch\n",
        "for itest in test_batch:\n",
        "  test_batch = preprocess(itest, augment=0)\n",
        "  test_maps, params_test = test_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6-ZXGhyjwmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ JL: batch generator\n",
        "def synth_batches():\n",
        "  for ibatch in train_batch:\n",
        "    yield preprocess(ibatch)\n",
        "\n",
        "batches = synth_batches()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdO_WMWAIYRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1db44b10-56a9-4293-f425-08e2e63a19c6"
      },
      "source": [
        "print (test_maps.shape, params_test.shape)\n",
        "print (valid_maps.shape, params_valid.shape)\n",
        "itrain_batch  = next(batches)\n",
        "print (itrain_batch[0].shape, itrain_batch[1].shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256, 1, 2000) (2000, 2)\n",
            "(256, 256, 1, 128) (128, 2)\n",
            "(256, 256, 1, 64) (64, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhtGSHyHQgdU",
        "colab_type": "code",
        "outputId": "772dc003-3366-4d63-b282-d5127e91f453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "############# JL: training\n",
        "\n",
        "num_classes = 2\n",
        "input_shape = (256, 256, 1, batch_size)\n",
        "num_steps = int(train_size/batch_size) ## number of steps per epoch\n",
        "num_epochs = 50\n",
        "step_size = 0.0001\n",
        "\n",
        "init_fun, predict_fun = ResNet50(num_classes)\n",
        "# _, init_params = init_fun(rng_key, input_shape)\n",
        "######### JL to do: change to grab the newest file automatically\n",
        "init_params = loadmodel(model_path+'jax_resnet_model_epoch0_stepsize0.0001.pickle')\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size) ##### seems to work best\n",
        "# opt_init, opt_update, get_params = optimizers.adagrad(step_size) #### seems to work\n",
        "# opt_init, opt_update, get_params = optimizers.sgd(step_size) ### doesn't work well\n",
        "# opt_init, opt_update, get_params = optimizers.nesterov(step_size, mass=0.9) ### doesn't work well\n",
        "# opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=0.9) ## doesn't work well\n",
        "\n",
        "@jit\n",
        "def update(i, opt_state, batch):\n",
        "  params = get_params(opt_state)\n",
        "  return opt_update(i, grad(loss)(params, batch), opt_state)\n",
        "\n",
        "opt_state = opt_init(init_params)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  for istep in range(num_steps):  \n",
        "    this_batch=next(batches)\n",
        "    opt_state = update(istep, opt_state, this_batch)\n",
        "    if istep%50==0:\n",
        "      epoch_time = (time.time() - start_time)/60.0 ## minutes\n",
        "      test_NNparams = get_params(opt_state)\n",
        "      train_loss = loss(test_NNparams, this_batch)\n",
        "      valid_loss = loss(test_NNparams, valid_batch)\n",
        "      print(\"Epoch {} at {:0.2f}m, train loss: {:0.2f}, validation loss: {:0.2f}\".format(epoch, \n",
        "                                                   epoch_time, train_loss, valid_loss))\n",
        "  fn_model=model_path+'jax_resnet_model_epoch{:0.0f}_stepsize{}.pickle'.format(epoch, step_size)\n",
        "  savemodel(test_NNparams, fn_model,overwrite=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 at 0.27m, train loss: 15566.53, validation loss: 30622.77\n",
            "Epoch 0 at 0.64m, train loss: 10.34, validation loss: 20.45\n",
            "Epoch 0 at 1.01m, train loss: 2.08, validation loss: 4.40\n",
            "Epoch 0 at 1.38m, train loss: 2.06, validation loss: 4.08\n",
            "Epoch 0 at 1.76m, train loss: 1.98, validation loss: 4.07\n",
            "Epoch 0 at 2.13m, train loss: 2.36, validation loss: 4.08\n",
            "Epoch 1 at 2.38m, train loss: 2.39, validation loss: 4.10\n",
            "Epoch 1 at 2.75m, train loss: 2.28, validation loss: 4.00\n",
            "Epoch 1 at 3.13m, train loss: 2.24, validation loss: 4.03\n",
            "Epoch 1 at 3.50m, train loss: 1.61, validation loss: 4.03\n",
            "Epoch 1 at 3.87m, train loss: 1.73, validation loss: 4.03\n",
            "Epoch 1 at 4.24m, train loss: 2.32, validation loss: 4.13\n",
            "Epoch 2 at 4.49m, train loss: 2.21, validation loss: 4.10\n",
            "Epoch 2 at 4.86m, train loss: 1.90, validation loss: 3.99\n",
            "Epoch 2 at 5.24m, train loss: 2.05, validation loss: 4.04\n",
            "Epoch 2 at 5.61m, train loss: 2.17, validation loss: 4.00\n",
            "Epoch 2 at 5.98m, train loss: 1.96, validation loss: 4.01\n",
            "Epoch 2 at 6.35m, train loss: 2.08, validation loss: 4.26\n",
            "Epoch 3 at 6.61m, train loss: 1.95, validation loss: 4.26\n",
            "Epoch 3 at 6.99m, train loss: 2.09, validation loss: 3.96\n",
            "Epoch 3 at 7.36m, train loss: 2.13, validation loss: 4.07\n",
            "Epoch 3 at 7.73m, train loss: 2.06, validation loss: 3.94\n",
            "Epoch 3 at 8.10m, train loss: 2.13, validation loss: 4.36\n",
            "Epoch 3 at 8.48m, train loss: 2.10, validation loss: 3.88\n",
            "Epoch 4 at 8.73m, train loss: 1.95, validation loss: 4.11\n",
            "Epoch 4 at 9.10m, train loss: 1.91, validation loss: 3.87\n",
            "Epoch 4 at 9.47m, train loss: 2.19, validation loss: 4.16\n",
            "Epoch 4 at 9.85m, train loss: 1.93, validation loss: 4.10\n",
            "Epoch 4 at 10.22m, train loss: 1.74, validation loss: 3.85\n",
            "Epoch 4 at 10.59m, train loss: 2.39, validation loss: 4.21\n",
            "Epoch 5 at 10.85m, train loss: 1.73, validation loss: 4.15\n",
            "Epoch 5 at 11.22m, train loss: 1.80, validation loss: 3.89\n",
            "Epoch 5 at 11.60m, train loss: 1.84, validation loss: 4.30\n",
            "Epoch 5 at 11.97m, train loss: 2.13, validation loss: 4.03\n",
            "Epoch 5 at 12.34m, train loss: 1.81, validation loss: 3.81\n",
            "Epoch 5 at 12.71m, train loss: 2.16, validation loss: 4.22\n",
            "Epoch 6 at 12.97m, train loss: 2.03, validation loss: 3.96\n",
            "Epoch 6 at 13.34m, train loss: 1.91, validation loss: 3.78\n",
            "Epoch 6 at 13.71m, train loss: 2.21, validation loss: 4.16\n",
            "Epoch 6 at 14.08m, train loss: 2.03, validation loss: 3.90\n",
            "Epoch 6 at 14.46m, train loss: 1.75, validation loss: 4.01\n",
            "Epoch 6 at 14.83m, train loss: 1.74, validation loss: 3.98\n",
            "Epoch 7 at 15.05m, train loss: 2.07, validation loss: 4.26\n",
            "Epoch 7 at 15.46m, train loss: 1.75, validation loss: 4.00\n",
            "Epoch 7 at 15.83m, train loss: 2.17, validation loss: 3.77\n",
            "Epoch 7 at 16.20m, train loss: 1.60, validation loss: 3.75\n",
            "Epoch 7 at 16.57m, train loss: 1.77, validation loss: 3.83\n",
            "Epoch 7 at 16.95m, train loss: 2.43, validation loss: 3.94\n",
            "Epoch 8 at 17.17m, train loss: 2.06, validation loss: 4.68\n",
            "Epoch 8 at 17.58m, train loss: 1.87, validation loss: 4.03\n",
            "Epoch 8 at 17.95m, train loss: 1.93, validation loss: 3.77\n",
            "Epoch 8 at 18.32m, train loss: 1.30, validation loss: 3.72\n",
            "Epoch 8 at 18.69m, train loss: 1.69, validation loss: 3.84\n",
            "Epoch 8 at 19.07m, train loss: 1.84, validation loss: 3.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuUrtvAgPpa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_NNparams = loadmodel(fn_model)\n",
        "\n",
        "params_pred = predict_fun(test_NNparams, valid_maps)\n",
        "params_input = params_valid\n",
        "true_s8=S8_gen(params_valid)\n",
        "pred_s8=S8_gen(params_pred)\n",
        "\n",
        "figure(figsize=(14,4))\n",
        "subplot(1,3,1)\n",
        "scatter(params_input.T[0], params_input.T[1],color='r')\n",
        "title('input parameters')\n",
        "xlabel('om')\n",
        "ylabel('sigma8')\n",
        "subplot(1,3,2)\n",
        "scatter(params_pred.T[0], params_pred.T[1],color='b')\n",
        "title('output parameters')\n",
        "xlabel('om')\n",
        "ylabel('sigma8')\n",
        "subplot(1,3,3)\n",
        "scatter(true_s8, pred_s8, color='k')\n",
        "xlabel('input S8')\n",
        "ylabel('predicted S8')\n",
        "ylim(0,1)\n",
        "title('comparison')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ffEbGOb8Ce2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ check  parameter shape\n",
        "# j=0\n",
        "# for iparam in trained_params:\n",
        "#   print ('layer:',j, '/ parameter shape:', array(iparam).shape)\n",
        "#   #print (iparam)\n",
        "#   j+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88O_83W1TTR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############# test data save and restart - pass\n",
        "# step_size=0.01\n",
        "# opt_init, opt_update, get_params = optimizers.adam(step_size) ##### seems to work best\n",
        "\n",
        "# _, init_params = init_fun(rng_key, input_shape)\n",
        "# savemodel(init_params, model_path+'test2')\n",
        "# testmodel = loadmodel(model_path+'test2.pickle')\n",
        "# opt_init(testmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIsGpz5SLQK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## test file save -- pass ############\n",
        "## saved as npy, and can be loaded and use as model prediction directly\n",
        "# np.save(model_path+'test', test_params)\n",
        "# test_model = np.load(model_path+'test.npy',allow_pickle=1)\n",
        "# loss(test_model, this_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00VJ2TzkaqxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########### test generator output shape - pass\n",
        "# final_NNparams = get_params(opt_state)\n",
        "# np.save(model_path+'resnet_params_{:.0f}epochs_{:.2f}stepsize.npy'.format(epoch+1, step_size), final_NNparams)\n",
        "\n",
        "# predictions = predict_fun(final_NNparams, kappa_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpGM1HQq8YUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}